{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05101167",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd Camera/Yolov5_DeepSort_Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f43e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# limit the number of cpus used by high performance libraries\n",
    "import os\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"VECLIB_MAXIMUM_THREADS\"] = \"1\"\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, './yolov5')\n",
    "\n",
    "from yolov5.models.experimental import attempt_load\n",
    "from yolov5.utils.downloads import attempt_download\n",
    "from yolov5.models.common import DetectMultiBackend\n",
    "from yolov5.utils.datasets import LoadImages, LoadStreams\n",
    "from yolov5.utils.general import LOGGER, check_img_size, non_max_suppression, scale_coords, check_imshow, xyxy2xywh\n",
    "from yolov5.utils.torch_utils import select_device, time_sync\n",
    "from yolov5.utils.plots import Annotator, colors\n",
    "from deep_sort_pytorch.utils.parser import get_config\n",
    "from deep_sort_pytorch.deep_sort import DeepSort\n",
    "import argparse\n",
    "import os\n",
    "import platform\n",
    "import shutil\n",
    "import time\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import skvideo.io\n",
    "import face_recognition\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "## dataset loading\n",
    "from utils.augmentations import Albumentations, augment_hsv, copy_paste, letterbox, mixup, random_perspective\n",
    "from threading import Thread\n",
    "from queue import Queue\n",
    "from imutils.video import VideoStream\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675d0383",
   "metadata": {},
   "outputs": [],
   "source": [
    "### for video Streaming\n",
    "class FileVideoStream:\n",
    "    def __init__(self, path, queueSize=256):\n",
    "        # initialize the file video stream along with the boolean\n",
    "        # used to indicate if the thread should be stopped or not\n",
    "        self.stream = cv2.VideoCapture(path)\n",
    "        self.stopped = False\n",
    "        # initialize the queue used to store frames read from\n",
    "        # the video file\n",
    "        self.Q = Queue(maxsize=queueSize)\n",
    "        \n",
    "    def start(self):\n",
    "        # start a thread to read frames from the file video stream\n",
    "        t = Thread(target=self.update, args=())\n",
    "        t.daemon = True\n",
    "        t.start()\n",
    "        return self\n",
    "    \n",
    "    def update(self):\n",
    "        # keep looping infinitely\n",
    "        c=0\n",
    "        while True:\n",
    "            c+=1\n",
    "            # if the thread indicator variable is set, stop the\n",
    "            # thread\n",
    "            if self.stopped:\n",
    "                return\n",
    "            \n",
    "            \n",
    "            # otherwise, ensure the queue has room in it\n",
    "            if not self.Q.full():\n",
    "                # read the next frame from the file\n",
    "                (grabbed, frame) = self.stream.read()\n",
    "\n",
    "                # if the `grabbed` boolean is `False`, then we have\n",
    "                # reached the end of the video file\n",
    "                #if c%2==0:\n",
    "                if grabbed:\n",
    "                    self.Q.put(frame)\n",
    "                # add the frame to the queue\n",
    "                \n",
    "                if self.Q.qsize()>192 and c%2==0:\n",
    "                    self.Q.get()\n",
    "                    continue\n",
    "\n",
    "                if self.Q.qsize()>96 and c%3==0:\n",
    "                    self.Q.get()\n",
    "                    continue\n",
    "                    \n",
    "                if c%5==0:\n",
    "                    self.Q.get()\n",
    "\n",
    "\n",
    "\n",
    "              \n",
    "    def read(self):\n",
    "        # return next frame in the queue\n",
    "        return self.Q.get()\n",
    "    \n",
    "    def more(self):\n",
    "        # return True if there are still frames in the queue\n",
    "        return self.Q.qsize() > 0\n",
    "    \n",
    "    def stop(self):\n",
    "        # indicate that the thread should be stopped\n",
    "        self.stopped = True\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22db989d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_fps(new_frame_time,prev_frame_time):\n",
    "    #######################-FPS-########################\n",
    "    \n",
    "    # Calculating the fps\n",
    "    new_frame_time = time.time()\n",
    "\n",
    "    # fps will be number of frame processed in given time frame\n",
    "    # since their will be most of time error of 0.001 second\n",
    "    # we will be subtracting it to get more accurate result\n",
    "    fps = 1/(new_frame_time-prev_frame_time)\n",
    "    prev_frame_time = new_frame_time\n",
    "\n",
    "    # converting the fps into integer\n",
    "    fps = int(fps)\n",
    "\n",
    "    # converting the fps to string so that we can display it on frame\n",
    "    # by using putText function\n",
    "    fps = str(fps)\n",
    "    #cv2.putText(frame, str(int(fps)), (7, 30), cv2.FONT_HERSHEY_DUPLEX, 1, (0, 0, 255), 3)\n",
    "\n",
    "    return fps,prev_frame_time\n",
    "    ######################################################\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e37354b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Person_left_the_frame(tracks,Detected_Faces,counts):\n",
    "    remove=[]\n",
    "    for t in tracks:\n",
    "        if t not in deepsort.tracker.tracks:\n",
    "            if t.track_id in Unkhown_Faces and t.track_id not in Detected_Faces:\n",
    "                remove.append(t.track_id)\n",
    "                Detected_Faces.update({t.track_id:\"Unkhown_Face\"})\n",
    "                counts.append(t.track_id)\n",
    "                counts=list(set(counts))\n",
    "                \n",
    "    return remove\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550325ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Not_Recognized(Unkhown_Faces,Detected_Faces,counts):\n",
    "    remove=[]\n",
    "    for key in Unkhown_Faces:\n",
    "        if Unkhown_Faces[key]>=60:\n",
    "            remove.append(key)\n",
    "            Detected_Faces.update({key:\"Unkhown_Face\"})\n",
    "            counts.append(key)\n",
    "            counts=list(set(counts))\n",
    "            \n",
    "    return remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5688b4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def face_locations_mp(results):\n",
    "    face_locations=[]\n",
    "    for detection in results.detections:\n",
    "        location=[]\n",
    "        #print(detection.location_data.relative_bounding_box)\n",
    "        xmin=detection.location_data.relative_bounding_box.xmin\n",
    "        if xmin<0:\n",
    "            xmin=0\n",
    "        ymin=detection.location_data.relative_bounding_box.ymin\n",
    "        if ymin<0:\n",
    "            ymin=0\n",
    "        xmax=xmin+detection.location_data.relative_bounding_box.width\n",
    "        if xmax<0:\n",
    "            xmax=0\n",
    "        ymax=ymin+detection.location_data.relative_bounding_box.height\n",
    "        if ymax<0:\n",
    "            ymax=0\n",
    "\n",
    "        h=detection.location_data.relative_bounding_box.height*Check_face.shape[0]\n",
    "        w=detection.location_data.relative_bounding_box.width*Check_face.shape[1]\n",
    "\n",
    "        # order of face locations for face_recognition if model is 0\n",
    "        location=(int(ymin*Check_face.shape[0]),int(xmax*Check_face.shape[1])-int(ymax*Check_face.shape[1]*0.14),int(ymax*Check_face.shape[0])-int(ymax*Check_face.shape[0]*0.146),int(xmin*Check_face.shape[1]))\n",
    "        \n",
    "        # order of face locations for face_recognition if model is 0\n",
    "        \n",
    "        #location=(int(ymin*Check_face.shape[0]+h*0.07775),int(xmax*Check_face.shape[1]-w*0.0706),int(ymax*Check_face.shape[0]-h*0.053),int(xmin*Check_face.shape[1]+w*0.03533))\n",
    "        \n",
    "        face_locations.append(location)\n",
    "    \n",
    "    return face_locations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b775e421",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_face(face_encodings,ALL,Unkhown_Faces,Detected_Faces,counts):\n",
    "    Distances=[]\n",
    "    Names=[]\n",
    "    for face_encoding in face_encodings:\n",
    "        for key in ALL:\n",
    "            F=0\n",
    "            distances=[]\n",
    "            for E in ALL[key]:\n",
    "\n",
    "                # See if the face is a match for the known face(s)\n",
    "                \n",
    "                name = \"Unknown\"\n",
    "\n",
    "                face_distance = face_recognition.face_distance([ALL[key][E]], face_encoding)\n",
    "                distances.append(face_distance)\n",
    "\n",
    "\n",
    "            #print(np.mean(distances))\n",
    "            Distances.append(np.mean(distances))\n",
    "            Names.append(key)\n",
    "\n",
    "        best_match_index = np.argmin(Distances)\n",
    "        #print(Distances)\n",
    "        #print(Distances[best_match_index])\n",
    "\n",
    "        if Distances[best_match_index]<=0.5:\n",
    "            name=Names[best_match_index]\n",
    "            #print(name)\n",
    "            if id in Unkhown_Faces:\n",
    "                del Unkhown_Faces[id]\n",
    "            Detected_Faces.update({id:name})\n",
    "            counts.append(id)\n",
    "            counts=list(set(counts))\n",
    "            \n",
    "        if name == \"Unknown\":\n",
    "            name=name\n",
    "            if id in Unkhown_Faces:\n",
    "                Unkhown_Faces.update({id:Unkhown_Faces[id]+1})\n",
    "            else:\n",
    "                if id not in Detected_Faces:\n",
    "                    Unkhown_Faces.update({id:0})\n",
    "                    ALL_FACES.append(name)\n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285f7931",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "## Reading Encodings\n",
    "\n",
    "import pickle\n",
    "#a_file = open(\"Encodings.pkl\", \"rb\")\n",
    "a_file = open(\"Face_Encodings.pkl\", \"rb\")\n",
    "ALL= pickle.load(a_file)\n",
    "print(ALL.keys())\n",
    "print(ALL['Faizan'].keys())\n",
    "print(ALL['Muneeb'].keys())\n",
    "print(ALL['Talha'].keys())\n",
    "\n",
    "\n",
    "# Initialize some variables\n",
    "face_locations = []\n",
    "face_encodings = []\n",
    "face_names = []\n",
    "\n",
    "\n",
    "\n",
    "### Variables used\n",
    "\n",
    "Detected_Faces={   \n",
    "}\n",
    "\n",
    "Unkhown_Faces={\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "Time_Stamps={\n",
    "    \n",
    "}\n",
    "\n",
    "Logs={\n",
    "    \n",
    "}\n",
    "\n",
    "counts=[]\n",
    "#name=\"not recognized\"\n",
    "ALL_FACES=[]\n",
    "tracks=[]\n",
    "\n",
    "# for fps calc\n",
    "\n",
    "# used to record the time when we processed last frame\n",
    "prev_frame_time = 0\n",
    " \n",
    "# used to record the time at which we processed current frame\n",
    "new_frame_time = 0\n",
    "\n",
    "# save path and fps\n",
    "video_save_path = \"Test_Output.mp4\"\n",
    "\n",
    "# create writer using FFmpegWriter\n",
    "fps = 20\n",
    "writer = skvideo.io.FFmpegWriter(video_save_path, inputdict={'-r': str(fps)},outputdict={'-r': str(fps), '-c:v': 'libx264', '-preset': 'ultrafast', '-pix_fmt': 'yuv444p'})\n",
    "\n",
    "        \n",
    "## Face Detector\n",
    "\n",
    "import mediapipe as mp\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    ### Setup\n",
    "    deep_sort_weights='deep_sort_pytorch/deep_sort/deep/checkpoint/ckpt.t7'\n",
    "    yolo_weights='yolov5/weights/crowdhuman_yolov5m.pt'\n",
    "    config_deepsort='deep_sort_pytorch/configs/deep_sort.yaml'\n",
    "\n",
    "    # initialize deepsort\n",
    "    cfg = get_config()\n",
    "    #print(cfg)\n",
    "    cfg.merge_from_file(config_deepsort)\n",
    "    attempt_download(deep_sort_weights, repo='mikel-brostrom/Yolov5_DeepSort_Pytorch')\n",
    "    deepsort = DeepSort(cfg.DEEPSORT.REID_CKPT,\n",
    "                        max_dist=cfg.DEEPSORT.MAX_DIST, min_confidence=cfg.DEEPSORT.MIN_CONFIDENCE,\n",
    "                        max_iou_distance=0.5,\n",
    "                        max_age=30, n_init=cfg.DEEPSORT.N_INIT, nn_budget=cfg.DEEPSORT.NN_BUDGET,\n",
    "                        use_cuda=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Initialize\n",
    "    Device=''\n",
    "    device = select_device(Device)\n",
    "    half=True\n",
    "    half &= device.type != 'cpu'  # half precision only supported on CUDA\n",
    "\n",
    "\n",
    "    # Load model\n",
    "    device = select_device(device)\n",
    "    model = DetectMultiBackend(yolo_weights, device=device, dnn=False)\n",
    "    stride, names, pt, jit, onnx = model.stride, model.names, model.pt, model.jit, model.onnx\n",
    "\n",
    "    imgsz=[640]\n",
    "    imgsz *= 2 if len(imgsz) == 1 else 1  # expand\n",
    "\n",
    "    imgsz = check_img_size(imgsz, s=stride)  # check image size\n",
    "\n",
    "\n",
    "\n",
    "    # Half\n",
    "    half &= pt and device.type != 'cpu'  # half precision only supported by PyTorch on CUDA\n",
    "    if pt:\n",
    "        model.model.half() if half else model.model.float()\n",
    "\n",
    "    vid_path, vid_writer = None, None\n",
    "    # Check if environment supports image displays\n",
    "    show_vid = check_imshow()\n",
    "\n",
    "    #for live stream\n",
    "    path='rtsp://admin:abc12345@192.168.1.64'\n",
    "    cudnn.benchmark = True  # set True to speed up constant image size inference\n",
    "    \n",
    "    #capture = cv2.VideoCapture(path)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Get names and colors\n",
    "    names = model.module.names if hasattr(model, 'module') else model.names\n",
    "\n",
    "\n",
    "    if pt and device.type != 'cpu':\n",
    "        model(torch.zeros(1, 3, *imgsz).to(device).type_as(next(model.model.parameters())))  # warmup\n",
    "    dt, seen = [0.0, 0.0, 0.0], 0\n",
    "    \n",
    "    \n",
    "    \n",
    "    ## Start Streaming\n",
    "    print(\"[INFO] starting video file thread...\")\n",
    "    fvs = FileVideoStream(\"rtsp://admin:abc12345@192.168.1.64\").start()\n",
    "    \n",
    "    time.sleep(1.0)\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        start_time=time.time()\n",
    "        \n",
    "        img0 = fvs.read()\n",
    "        \n",
    "        fps,prev_frame_time=calc_fps(new_frame_time,prev_frame_time)\n",
    "        \n",
    "        if cv2.waitKey(1) == ord('q'):  # q to quit\n",
    "            break\n",
    "        \n",
    "        #if img0 is not None:\n",
    "        im0s = cv2.resize(img0, (0, 0), fx=0.5,fy=0.5)\n",
    "\n",
    "        # Padded resize\n",
    "        img = letterbox(im0s, imgsz, stride=32 , auto=True)[0]\n",
    "\n",
    "        # Convert\n",
    "        img = img.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n",
    "        img = np.ascontiguousarray(img)\n",
    "\n",
    "        t1 = time_sync()\n",
    "        img = torch.from_numpy(img).to(device)\n",
    "        img = img.half() if half else img.float()  # uint8 to fp16/32\n",
    "        img /= 255.0  # 0 - 255 to 0.0 - 1.0\n",
    "        if img.ndimension() == 3:\n",
    "            img = img.unsqueeze(0)\n",
    "        t2 = time_sync()\n",
    "        dt[0] += t2 - t1\n",
    "\n",
    "        # Inference\n",
    "        opt_visualize=False\n",
    "        visualize = increment_path(save_dir / Path(path).stem, mkdir=True) if opt_visualize else False\n",
    "        pred = model(img, augment=False, visualize=visualize)\n",
    "        t3 = time_sync()\n",
    "        dt[1] += t3 - t2\n",
    "\n",
    "\n",
    "        # Apply NMS\n",
    "        pred = non_max_suppression(pred,0.7,0.5, 0 , False , max_det=1000)\n",
    "        dt[2] += time_sync() - t3\n",
    "        \n",
    "        # Process detections\n",
    "        for i, det in enumerate(pred):  # detections per image\n",
    "            seen += 1\n",
    "\n",
    "            #stream\n",
    "            p, im0= path, im0s.copy()\n",
    "\n",
    "            annotator = Annotator(im0, line_width=2, pil=not ascii)\n",
    "\n",
    "            if det is not None and len(det):\n",
    "\n",
    "                # Rescale boxes from img_size to im0 size\n",
    "                det[:, :4] = scale_coords(\n",
    "                    img.shape[2:], det[:, :4], im0.shape).round()\n",
    "\n",
    "                # Print results\n",
    "                for c in det[:, -1].unique():\n",
    "                    n = (det[:, -1] == c).sum()  # detections per class\n",
    "\n",
    "                xywhs = xyxy2xywh(det[:, 0:4])\n",
    "                confs = det[:, 4]\n",
    "                clss = det[:, 5]\n",
    "\n",
    "                # pass detections to deepsort\n",
    "                outputs = deepsort.update(xywhs.cpu(), confs.cpu(), clss.cpu(), im0)\n",
    "\n",
    "                #print(outputs)\n",
    "\n",
    "                ##############--Check if person has left the frame and still not reconized--###############\n",
    "\n",
    "                \n",
    "                remove=Person_left_the_frame(tracks,Detected_Faces,counts)\n",
    "\n",
    "                for r in remove:\n",
    "                    del Unkhown_Faces[r]\n",
    "\n",
    "\n",
    "                tracks=deepsort.tracker.tracks\n",
    "\n",
    "                ##########################################################################################\n",
    "\n",
    "\n",
    "                # draw boxes for visualization\n",
    "                if len(outputs) > 0:\n",
    "                    for j, (output, conf) in enumerate(zip(outputs, confs)): \n",
    "\n",
    "                        bboxes = output[0:4]\n",
    "                        id = output[4]\n",
    "                        cls = output[5]\n",
    "\n",
    "\n",
    "                        ##############--Check if person is already Recognized--###############\n",
    "\n",
    "                        if id in Detected_Faces:\n",
    "                            c = int(cls)  # integer class\n",
    "                            label = f'{id} {names[c]} {Detected_Faces[id]} {conf:.2f}'\n",
    "                            annotator.box_label(bboxes, label, color=colors(c, True))\n",
    "                            continue\n",
    "\n",
    "                        ########################################################################\n",
    "\n",
    "                        ##############--Check if person cannot be identified--##################\n",
    "                        \n",
    "                        remove=Not_Recognized(Unkhown_Faces,Detected_Faces,counts)\n",
    "                        \n",
    "                        for r in remove:\n",
    "                            del Unkhown_Faces[r]\n",
    "\n",
    "                        ########################################################################\n",
    "                        \n",
    "                        # croping detected person for face_detection\n",
    "                        \n",
    "                        #only getting center of bounding box to avoid multiple faces in a single box\n",
    "                        Check_face=im0[output[1]:output[3]-int(0.5*(abs(output[3]-output[1]))),output[0]+int(0.3*(abs(output[2]-output[0]))):output[2]-int(0.30*(abs(output[2]-output[0])))]\n",
    "                        Check_face = cv2.resize(Check_face, (0, 0), fx=2,fy=2)\n",
    "\n",
    "\n",
    "                        ##################################--Detections using media pipe--#####################################\n",
    "                        \n",
    "                        with mp_face_detection.FaceDetection(model_selection=0, min_detection_confidence=0.7) as face_detection:\n",
    "                            results = face_detection.process(cv2.cvtColor(Check_face, cv2.COLOR_BGR2RGB))\n",
    "                            \n",
    "                        ##################################--Detections using media pipe--#####################################\n",
    "                    \n",
    "                        ## move to next if face was not detected for a person\n",
    "                        if results.detections is None: \n",
    "                            ## loading time when person was detected\n",
    "                            now = datetime.now()\n",
    "                            if id not in Time_Stamps:\n",
    "                                Time_Stamps.update({id:now})\n",
    "                            continue\n",
    "      \n",
    "                        ## if face was detected\n",
    "                        else:\n",
    "                \n",
    "                            face_locations=face_locations_mp(results)\n",
    "                        \n",
    "                            face_encodings = face_recognition.face_encodings(Check_face, face_locations)\n",
    "                            \n",
    "                            verify_face(face_encodings,ALL,Unkhown_Faces,Detected_Faces,counts)\n",
    "                                \n",
    "                            c = int(cls)  # integer class\n",
    "                            label = f'{id} {names[c]} {name} {conf:.2f}'\n",
    "\n",
    "                            annotator.box_label(bboxes, label, color=colors(2, True))\n",
    "\n",
    "\n",
    "            else:\n",
    "                deepsort.increment_ages()\n",
    "\n",
    "            im0 = annotator.result()\n",
    "            \n",
    "            if show_vid:\n",
    "\n",
    "\n",
    "                cv2.putText(im0, str(len(counts)), (50 , 50), cv2.FONT_HERSHEY_DUPLEX, 2, (255, 255, 255), 2)\n",
    "\n",
    "                cv2.putText(im0, str(int(fps)), (7, 30), cv2.FONT_HERSHEY_DUPLEX, 1, (0, 0, 255), 3)\n",
    "\n",
    "                cv2.putText(im0, \"Queue Size: {}\".format(fvs.Q.qsize()),(10, 100), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "\n",
    "                cv2.imshow(p, im0)\n",
    "\n",
    "                im_bgr = cv2.cvtColor(im0, cv2.COLOR_RGB2BGR)\n",
    "                writer.writeFrame(im_bgr)\n",
    "                \n",
    "            #print(Unkhown_Faces)\n",
    "            #print(Detected_Faces)\n",
    "\n",
    "\n",
    "for key in Detected_Faces:\n",
    "    Logs.update({key: Detected_Faces[key]+\" , \"+str(Time_Stamps[key])})\n",
    "\n",
    "writer.close()\n",
    "cv2.destroyAllWindows()\n",
    "fvs.stop()\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f89348",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
